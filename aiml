LOGISTIC

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/logistic_regression/final_dataset/"


X=pd.read_excel(path+"X.xlsx")
Y=pd.read_excel(path+"Y.xlsx")


X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1)

X_train = X_train.values
Y_train = Y_train.values
X_test = X_test.values
Y_test = Y_test.values

X_train = X_train.T
Y_train = Y_train.reshape(1, X_train.shape[1])

X_test = X_test.T
Y_test = Y_test.reshape(1, X_test.shape[1])



def sigmoid(x):
    return 1/(1 + np.exp(-x))

def model(X, Y, learning_rate, iterations):
    
    m = X_train.shape[1]
    n = X_train.shape[0]
    
    W = np.zeros((n,1))
    B = 0
    
    cost_list = []
    
    for i in range(iterations):
        
        Z = np.dot(W.T, X) + B
        A = sigmoid(Z)
        
        # cost function
        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))
        
        # Gradient Descent
        dW = (1/m)*np.dot(A-Y, X.T)
        dB = (1/m)*np.sum(A - Y)
        
        W = W - learning_rate*dW.T
        B = B - learning_rate*dB
        
        # Keeping track of our cost function value
        cost_list.append(cost)
        
        if(i%(iterations/10) == 0):
            print("cost after ", i, "iteration is : ", cost)
        
    return W, B, cost_list


iterations = 100000
learning_rate = 0.0015
W, B, cost_list = model(X_train, Y_train, learning_rate = learning_rate, iterations = iterations)

plt.plot(np.arange(iterations), cost_list)
plt.show()


def predict(X, Y, W, B):
    
    Z = np.dot(W.T, X) + B
    A = sigmoid(Z)
    
    A = A > 0.5
    
    A = np.array(A, dtype = 'int64')
    
     
    return A
    

Y_pred=predict(X_test, Y_test, W, B)
Y_pred_l=Y_pred.flatten()
Y_pred_l=Y_pred_l.tolist()


Y_test_l=Y_test.flatten()
Y_test_l=Y_test_l.tolist()
result=0
for i in range(len(Y_test_l)):
    if Y_test_l[i]==Y_pred_l[i]:
        result=result+1

manual_accuracy=result/len(Y_test_l)
print("Accuracy using loop is: ",manual_accuracy*100)


confusion_matrix = confusion_matrix(Y_test_l, Y_pred_l)
f1_value=f1_score(Y_test_l,Y_pred_l)
acc=accuracy_score(Y_test_l,Y_pred_l)
print("F1 score is: ",f1_value*100)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)


LOGISTIC REG GRADIENT DESCENT
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2025/dataset/logistic_regression/final_dataset/"


X=pd.read_excel(path+"X.xlsx")
Y=pd.read_excel(path+"Y.xlsx")


X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1)

X_train = X_train.values
Y_train = Y_train.values
X_test = X_test.values
Y_test = Y_test.values

X_train = X_train.T
Y_train = Y_train.reshape(1, X_train.shape[1])

X_test = X_test.T
Y_test = Y_test.reshape(1, X_test.shape[1])



def sigmoid(x):
    return 1/(1 + np.exp(-x))

def model(X, Y, learning_rate, iterations):
    
    m = X_train.shape[1]
    n = X_train.shape[0]
    
    W = np.zeros((n,1))
    B = 0
    
    cost_list = []
    
    for i in range(iterations):
        
        Z = np.dot(W.T, X) + B
        A = sigmoid(Z)
        
        # cost function
        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))
        
        # Gradient Descent
        dW = (1/m)*np.dot(A-Y, X.T)
        dB = (1/m)*np.sum(A - Y)
        
        W = W - learning_rate*dW.T
        B = B - learning_rate*dB
        
        # Keeping track of our cost function value
        cost_list.append(cost)
        
        if(i%(iterations/10) == 0):
            print("cost after ", i, "iteration is : ", cost)
        
    return W, B, cost_list


iterations = 100000
learning_rate = 0.0015
W, B, cost_list = model(X_train, Y_train, learning_rate = learning_rate, iterations = iterations)

plt.plot(np.arange(iterations), cost_list)
plt.show()


def predict(X, Y, W, B):
    
    Z = np.dot(W.T, X) + B
    A = sigmoid(Z)
    
    A = A > 0.5
    
    A = np.array(A, dtype = 'int64')
    
     
    return A
    

Y_pred=predict(X_test, Y_test, W, B)
Y_pred_l=Y_pred.flatten()
Y_pred_l=Y_pred_l.tolist()


Y_test_l=Y_test.flatten()
Y_test_l=Y_test_l.tolist()
result=0
for i in range(len(Y_test_l)):
    if Y_test_l[i]==Y_pred_l[i]:
        result=result+1

manual_accuracy=result/len(Y_test_l)
print("Accuracy using loop is: ",manual_accuracy*100)


confusion_matrix = confusion_matrix(Y_test_l, Y_pred_l)
f1_value=f1_score(Y_test_l,Y_pred_l)
acc=accuracy_score(Y_test_l,Y_pred_l)
print("F1 score is: ",f1_value*100)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)

DECISION TREE CLASSIFIER
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2025/dataset/decision_tree_classifier/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.4,random_state=15)
decision_tree=DecisionTreeClassifier(max_depth=10)
decision_tree.fit(X_train, Y_train)
Y_pred=decision_tree.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
acc=accuracy_score(Y_test,Y_pred)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)


DECISION TREE REGRESSION
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeRegressor
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/decision_tree_regression/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
'''path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/decision_tree_regression/"
data=pd.read_csv(path+"diabetes.csv")
X=data.drop("Outcome",axis=1)
data1=pd.read_csv(path+"diabetes.csv")
data1.drop(data1.iloc[:,0:8],axis=1, inplace=True)
Y=data1
print(Y)'''

#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4,random_state=15)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.4,random_state=15)




print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
#log_reg=LogisticRegression(max_iter=10000)
decision_tree=DecisionTreeRegressor(max_depth=20)




decision_tree.fit(X_train, Y_train)
print(decision_tree.get_params())
Y_pred=decision_tree.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
#f1_value=f1_score(Y_test,Y_pred)
acc=accuracy_score(Y_test,Y_pred)
#print("F1 score is: ",f1_value*100)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)



KNN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/knn/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)


#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4,random_state=15)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.5,random_state=15)




print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
#log_reg=LogisticRegression(max_iter=10000)
knn_classifier=KNeighborsClassifier(n_neighbors=5)




knn_classifier.fit(X_train, Y_train)
print(knn_classifier.get_params())
Y_pred=knn_classifier.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
#f1_value=f1_score(Y_test,Y_pred)
acc=accuracy_score(Y_test,Y_pred)
#print("F1 score is: ",f1_value*100)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)


NAIVE BAYES
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2025/dataset/naive_bayes/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.5,random_state=15)
naive_bayes_clasiifier=GaussianNB()
naive_bayes_clasiifier.fit(X_train, Y_train)
Y_pred=naive_bayes_clasiifier.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
acc=accuracy_score(Y_test,Y_pred)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)


RANDOM FOREST CLASSIFIER
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/random_forest_classifier/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.4,random_state=15)
random_forest=RandomForestClassifier()
random_forest.fit(X_train, Y_train)
Y_pred=random_forest.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
acc=accuracy_score(Y_test,Y_pred)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)





RANDOM FOREST REGRESSOR
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/random_forest_classifier/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
'''path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/decision_tree_regression/"
data=pd.read_csv(path+"diabetes.csv")
X=data.drop("Outcome",axis=1)
data1=pd.read_csv(path+"diabetes.csv")
data1.drop(data1.iloc[:,0:8],axis=1, inplace=True)
Y=data1
print(Y)'''

#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4,random_state=15)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.4,random_state=15)




print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
#log_reg=LogisticRegression(max_iter=10000)
decision_tree=RandomForestRegressor()




decision_tree.fit(X_train, Y_train)
print(decision_tree.get_params())
Y_pred=decision_tree.predict(X_test)
result=0
#confusion_matrix = confusion_matrix(Y_test, Y_pred)
#f1_value=f1_score(Y_test,Y_pred)
mean_sq_error=mean_squared_error(Y_test,Y_pred)
#print("F1 score is: ",f1_value*100)
print("Value of mean square error is: ",mean_sq_error)
#print(confusion_matrix)



RNN
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from tensorflow.keras.utils import to_categorical
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2024/dataset/rnn/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)


#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4,random_state=15)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.2,random_state=15)




print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

Y_train=to_categorical(Y_train, num_classes=3)
Y_test=to_categorical(Y_test, num_classes=3)
X_train=np.array(X_train)
Y_train=np.array(Y_train)
X_test=np.array(X_test)
Y_test=np.array(Y_test)


X_train=X_train.reshape(-1,120,4)
Y_train=Y_train.reshape(-1,120,3)
X_test=X_test.reshape(-1,30,4)
Y_test=Y_test.reshape(-1,30,3)


model = Sequential()
model.add(SimpleRNN(units=64, input_shape=(120,4),return_sequences=True))  # Adjust units and input_shape as needed
model.add(Dense(3, activation='sigmoid'))  # For binary classification, adjust activation for other tasks
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Adjust loss and metrics for your task
model.fit(X_train, Y_train, epochs=120, batch_size=32, validation_data=(X_test, Y_test))

loss, accuracy = model.evaluate(X_test, Y_test)
print('Test Loss:', loss)
print('Test Accuracy:', accuracy)



#log_reg=LogisticRegression(max_iter=10000)
'''knn_classifier=KNeighborsClassifier(n_neighbors=5)




knn_classifier.fit(X_train, Y_train)
print(knn_classifier.get_params())
Y_pred=knn_classifier.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
#f1_value=f1_score(Y_test,Y_pred)
acc=accuracy_score(Y_test,Y_pred)
#print("F1 score is: ",f1_value*100)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)'''



SVM
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.preprocessing import LabelEncoder
path="C:/Users/UEM/OneDrive - University Of Engineering & Management/subject/AI/lab/2025/dataset/svm/"
data=pd.read_csv(path+"Iris_Dataset.csv")
X=data.drop("species",axis=1)
data1=pd.read_csv(path+"Iris_Dataset.csv")
data1.drop(data1.iloc[:,0:4],axis=1, inplace=True)
Y=data1
le = LabelEncoder()
labels = le.fit_transform(Y)
print(Y)
print(labels)
X_train,X_test,Y_train,Y_test=train_test_split(X,labels,test_size=0.5,random_state=15)
support_vector_machine=svm.SVC(kernel='linear', C=1.0) #kernels = ['linear', 'poly', 'rbf', 'sigmoid']
support_vector_machine.fit(X_train, Y_train)
Y_pred=support_vector_machine.predict(X_test)
result=0
confusion_matrix = confusion_matrix(Y_test, Y_pred)
acc=accuracy_score(Y_test,Y_pred)
print("Accuracy using function is: ",acc*100)
print(confusion_matrix)






























